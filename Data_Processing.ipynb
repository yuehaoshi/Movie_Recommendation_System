{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Processing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP+WTwfsLi01NnP0ZA5RqFX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuehaoshi/Movie_Recommendation_System/blob/main/Data_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UNrf5K1w1f7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm \n",
        "import re\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.impute import SimpleImputer\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Cleaning and Peocessing"
      ],
      "metadata": {
        "id": "8q_Un-Ynw2g_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_ori = pd.read_csv(\"https://raw.githubusercontent.com/Noam-91/Movie_Recommendation_system/main/netflix_titles.csv\")"
      ],
      "metadata": {
        "id": "DGUK9tXqxAJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_ori.head()\n",
        "data_ori.info()"
      ],
      "metadata": {
        "id": "oWMCAw02xGok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_ori.isnull().sum()"
      ],
      "metadata": {
        "id": "lMWjMtJ9xLlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Director, Cast and Country"
      ],
      "metadata": {
        "id": "IcRDEWEMxPQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tv_total = data_ori[data_ori['type']=='TV Show'].shape[0]\n",
        "missing_dir_tv = pd.isna(data_ori[data_ori['type']=='TV Show']['director']).sum()\n",
        "movie_total = data_ori[data_ori['type']=='Movie'].shape[0]\n",
        "missing_dir_movie = pd.isna(data_ori[data_ori['type']=='Movie']['director']).sum()\n",
        "class1 = [missing_dir_tv, missing_dir_movie]\n",
        "class2 = [tv_total-missing_dir_tv, movie_total-missing_dir_movie]\n",
        "\n",
        "plt.bar(range(2), class1, width=.5)\n",
        "plt.bar(range(2), class2, bottom=class1, width=.5)\n",
        "plt.xticks(range(2), ('TV Shows', 'Movies'))\n",
        "plt.legend(('Record without director ', 'Record with director'))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yipfGxG_xOdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_movie = data_ori[data_ori['type'] == 'Movie']\n",
        "data_tv = data_ori[data_ori['type'] == 'TV Show']\n",
        "data_movie.drop(columns='type',axis=1,inplace=True)\n",
        "data_tv.drop(columns='type',axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "p_IzylKnxU7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"prepare_country\" function: take a dataFrame, firstly add the possible countries with its corresponding director (if have any), then drop rows with 'director', 'cast' or 'country' to be null"
      ],
      "metadata": {
        "id": "3FSwqsE_xf1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_country(dataFrame):\n",
        "    #Most movies have director and cast, so drop rows with empty director and cast first\n",
        "    #Since most of TV shows do not have director, only drop empty cast for TV show\n",
        "    dataFrame = dataFrame.dropna(subset = ['cast'])\n",
        "    #Create a dictioary containing each director and their most frequent corresponding country, d_c_pair\n",
        "    directors = dataFrame['director'].str.split(', ')\n",
        "    directors1 = directors.dropna()\n",
        "    director_set = set()\n",
        "    for row in directors1:\n",
        "        if pd.notnull(row[0]):\n",
        "            for director in row:\n",
        "                if director not in director_set:\n",
        "                    director_set.add(director)  \n",
        "    d_c = Counter()\n",
        "    for idx, row in dataFrame.iterrows():\n",
        "        if pd.notna(row['director']) and pd.notna(row['country']):\n",
        "            director = row['director'].split(', ')\n",
        "            country = row['country'].split(', ')\n",
        "            for d in director:\n",
        "                for c in country:\n",
        "                    d_c[(d, c)] += 1\n",
        "    d_c_pair = {}\n",
        "    for i in director_set:\n",
        "        max = 0\n",
        "        max_c = None\n",
        "        for j in d_c:\n",
        "            d = j[0]\n",
        "            c = j[1]\n",
        "            if d == i:\n",
        "                if d_c[j] >= max:\n",
        "                    max = d_c[j]\n",
        "                    max_c = c\n",
        "        if max_c:\n",
        "            d_c_pair[i] = max_c\n",
        "    data = dataFrame.copy()\n",
        "    data['director'] = data['director'].fillna('nan')\n",
        "    data['country'] = data['country'].fillna(np.nan)\n",
        "    data['director'] = data['director'].str.split(', ')\n",
        "    condition = (pd.isna(data['country'])) & ('nan' not in data['director'])\n",
        "    #If a row has director but no country, put the director's corresponding country (if exists) into the row\n",
        "    for row in range(data.shape[0]):\n",
        "        country_column = list(data.columns).index('country')\n",
        "        director_column = list(data.columns).index('director')\n",
        "        if (pd.isna(data.iloc[row, country_column])) & (data.iloc[row, director_column][0] != 'nan') & (data.iloc[row, director_column][0] in d_c_pair):\n",
        "            data.iloc[row, country_column] = d_c_pair[data.iloc[row, director_column][0]] \n",
        "    #Delete the rows without country\n",
        "    data = data.dropna(subset = ['country'])\n",
        "    return data"
      ],
      "metadata": {
        "id": "_Lf2pjNuxkFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_movie = data_movie.dropna(subset = ['director'])\n",
        "data_movie = prepare_country(data_movie)\n",
        "data_tv = prepare_country(data_tv)\n",
        "print(data_movie.shape[0])\n",
        "print(data_tv.shape[0])\n",
        "plt.pie([data_movie.shape[0],data_tv.shape[0]], labels=['Movie',\"TV Shows\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GHiV1rlwxk2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(dataframe,prefix):\n",
        "    if prefix != \"director\":\n",
        "        classes = dataframe[prefix].str.split(', ')\n",
        "    else:\n",
        "        classes = dataframe[prefix]\n",
        "    class_set = set()\n",
        "    for row in classes:\n",
        "        #print(row)\n",
        "        for cls in row:\n",
        "            if cls not in class_set:\n",
        "                class_set.add(cls)\n",
        "    df = pd.DataFrame()\n",
        "    for cls in class_set:\n",
        "        df[str(prefix+\"_\"+cls)] = dataframe[prefix].apply(lambda x: 1 if x.__contains__(cls) else 0)\n",
        "    dataframe = pd.concat([dataframe,df], axis=1)\n",
        "    dataframe.drop(columns=[prefix],axis=1, inplace=True)\n",
        "    return dataframe"
      ],
      "metadata": {
        "id": "Eb5VEoW-xopC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare to directly deleting all rows containing null in 'director', 'cast' or 'country', we have more rows kept in our modified database, which is because of our added country information inferred from the director's corresponding frequent country"
      ],
      "metadata": {
        "id": "vVEY_6JrxrmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_movie.isna().sum()"
      ],
      "metadata": {
        "id": "ywKRaSsWxpZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_movie"
      ],
      "metadata": {
        "id": "J2r8rONlxu1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_movie = one_hot(data_movie, 'director')\n",
        "data_movie = one_hot(data_movie, 'country')\n",
        "data_movie = one_hot(data_movie, 'cast')\n",
        "data_tv = one_hot(data_tv, 'cast') \n",
        "data_tv = one_hot(data_tv, 'country')"
      ],
      "metadata": {
        "id": "vcmezcQExy0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_movie.info()"
      ],
      "metadata": {
        "id": "-44NPp2Wx02i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_tv.info()"
      ],
      "metadata": {
        "id": "sa782UZGx1Xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Rating"
      ],
      "metadata": {
        "id": "b4Ic8vIXx3od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rating_series = data_ori['rating']\n",
        "print(rating_series.describe())\n",
        "print()\n",
        "rating_series.value_counts()"
      ],
      "metadata": {
        "id": "htFX8XHux5eT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are three mistakes in the record. The 'duration' was mistakenly written into 'rating'. So we need to switch the value between these two columns. After that, we also do One-Hot encoding for 'rating'."
      ],
      "metadata": {
        "id": "2J8JxJeWx-Xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_rating(dataset):\n",
        "    row_idx = np.where(pd.isna(dataset['duration']))[0]\n",
        "    if row_idx.size == 0: return dataset\n",
        "    dur_idx = list(dataset.columns).index('duration')\n",
        "    rat_idx = list(dataset.columns).index('rating')\n",
        "    dataset.iloc[row_idx,dur_idx] = dataset.iloc[row_idx,rat_idx]\n",
        "    dataset.iloc[row_idx,rat_idx] = np.nan\n",
        "    data_rating = pd.get_dummies(dataset['rating'], drop_first=True)\n",
        "    dataset = pd.concat([data_rating,dataset],axis=1)\n",
        "    dataset.dropna(subset=['rating'])\n",
        "    dataset.drop(columns=['rating'],axis=1, inplace=True)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "P7wxt2fbx77n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_movie = prepare_rating(data_movie)\n",
        "data_tv = prepare_rating(data_tv)"
      ],
      "metadata": {
        "id": "OC9GM8TcyBGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Duration"
      ],
      "metadata": {
        "id": "a7ncyucryDjp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since duration contains two types of values: xxx min and xxx Seasons, we need to split them and put them into bins, because a classification makes more sense than regression in this task"
      ],
      "metadata": {
        "id": "wffKRZaeyIoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "movie_dur_col = data_movie['duration'].apply(lambda x: x.split(\" min\")[0] if x.__contains__(\"min\") else 0)\n",
        "seasons_col = data_tv['duration'].apply(lambda x: x.split(\" Season\")[0] if x.__contains__(\"Season\") else 0)\n",
        "seasons_col = seasons_col.astype(int)\n",
        "movie_dur_col = movie_dur_col.astype(int)"
      ],
      "metadata": {
        "id": "7fvRBuSOyFP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seasons_col[(0<seasons_col)].hist(bins=np.arange(17), grid=False)"
      ],
      "metadata": {
        "id": "qbrul12IyLgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_dur_col = movie_dur_col.astype(int)\n",
        "movie_dur_col[(0<movie_dur_col)].hist(bins=[0,30,60,90,120,150,200,312], grid=False)"
      ],
      "metadata": {
        "id": "dKl2TL83yNc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_duration(dataset, is_movie):\n",
        "    if is_movie:\n",
        "        movie_dur_col = dataset['duration'].apply(lambda x: x.split(\" min\")[0] if x.__contains__(\"min\") else 0)\n",
        "        movie_dur_col = movie_dur_col.astype(int)\n",
        "        df_movie_duration = pd.DataFrame()\n",
        "        bins = [1,30,60,90,120,150,200,312]\n",
        "        for x in range(len(bins)-1):\n",
        "            col_name = \"movie_duration_[\"+str(bins[x])+\":\"+str(bins[x+1])+\"]\"\n",
        "            df_movie_duration[col_name] = movie_dur_col.apply(lambda y: 1 if bins[x]<=y and bins[x+1]>y else 0)\n",
        "        dataset.drop(columns=['duration'],axis=1,inplace=True)\n",
        "        return pd.concat([dataset,df_movie_duration],axis=1)\n",
        "    else:\n",
        "        seasons_col = dataset['duration'].apply(lambda x: x.split(\" Season\")[0] if x.__contains__(\"Season\") else 0)\n",
        "        seasons_col = seasons_col.astype(int)\n",
        "        df_seasons = pd.DataFrame()\n",
        "        bins = [1,3,17]\n",
        "        for x in range(len(bins)-1):\n",
        "            col_name = \"seasons_[\"+str(bins[x])+\":\"+str(bins[x+1])+\"]\"\n",
        "            df_seasons[col_name] = seasons_col.apply(lambda y: 1 if bins[x]<=y and bins[x+1]>y else 0)\n",
        "        dataset.drop(columns=['duration'],axis=1,inplace=True)\n",
        "        return pd.concat([dataset,df_seasons],axis=1)"
      ],
      "metadata": {
        "id": "diU244UoyQCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_seasons = prepare_duration(data_tv,False)\n",
        "data_movie_duration = prepare_duration(data_movie,True)"
      ],
      "metadata": {
        "id": "mmx6w4p6ySoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Genres"
      ],
      "metadata": {
        "id": "lD-bm5pmydKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A movie may belongs to multiple genre which is stacked in one column named as 'listed_in', we need to do one-hot encoding first."
      ],
      "metadata": {
        "id": "ta7HJwixyauH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "genres = data_ori['listed_in'].str.split(', ')\n",
        "genres_set = set()\n",
        "for row in genres:\n",
        "    for genre in row:\n",
        "        if genre not in genres_set:\n",
        "            genres_set.add(genre)\n",
        "data = data_ori.copy()\n",
        "for genre in genres_set:\n",
        "    data[genre] = data['listed_in'].apply(lambda x: 1 if x.__contains__(genre) else 0)\n",
        "data = data.drop(columns=['listed_in'],axis=1)\n",
        "data.columns[11:]"
      ],
      "metadata": {
        "id": "rkttJvVSyfLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the 'type' feature has defined the object as TV shows or Movies, we need to combine some of the genres to reduce correlation.\n",
        "'Docuseries' + 'Documentaries' --> 'Documentaries'\n",
        "'TV Mysteries' --> 'Mysteries'\n",
        "'TV Horror'+'Horror Movies' --> 'Horror'\n",
        "'Thrillers'+'TV Thrillers' --> 'Thrillers'\n",
        "'Spanish-Language TV Shows' --> 'Spanish'\n",
        "'LGBTQ Movies' --> 'LGBTQ'\n",
        "'Stand-Up Comedy & Talk Shows' + 'Stand-Up Comedy' --> 'Stand-Up Comedy'\n",
        "'TV Action & Adventure' + 'Action & Adventure' --> Action & Adventure\n",
        "'TV Shows' -- deleted\n",
        "'Science & Nature TV'\n",
        "'Crime TV Shows' --> crime\n",
        "'British TV Shows' --> british\n",
        "'Reality TV' --> reality\n",
        "'Classic Movies' --> classic\n",
        "'Sports Movies' --> sports\n",
        "'Comedies' -- Keep\n",
        "'Romantic Movies'+'Romantic TV Shows' --> Romantic\n",
        "'International Movies'+'International TV Shows' --> international\n",
        "'Dramas'+'TV Dramas' --> Dramas\n",
        "'TV Sci-Fi & Fantasy'+'Sci-Fi & Fantasy' --> sci-fi & Fantasy\n",
        "'Cult Movies'+ Classic & Cult TV --> cult\n",
        "'Children & Family Movies'--> 'Children & Family'\n",
        "'Music & Musicals'-- Keep\n",
        "'Korean TV Shows' --> Korean\n",
        "'Kids' TV' --> Kids\n",
        "'Movies'-- deleted\n",
        "'Anime Series'+'Anime Features' --> Anime\n",
        "'Teen TV Shows' --> Teen"
      ],
      "metadata": {
        "id": "fiwANY5Kyq81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_genre(dataset):\n",
        "    data_copy = dataset.copy()\n",
        "    genre_series = data_copy['listed_in']\n",
        "    genres = genre_series.str.split(', ')\n",
        "    genres_set = set()\n",
        "    for row in genres:\n",
        "        for genre in row:\n",
        "            if genre not in genres_set:\n",
        "                genres_set.add(genre)\n",
        "    df = pd.DataFrame()\n",
        "    for genre in genres_set:\n",
        "      if genre==\"Movies\" or genre==\"TV Shows\":\n",
        "        continue\n",
        "      df[genre] = genre_series.apply(lambda x: 1 if x.__contains__(genre) else 0)\n",
        "    data_copy.drop(columns=['listed_in'],axis=1,inplace=True)\n",
        "    return pd.concat([data_copy,df],axis=1), df.columns"
      ],
      "metadata": {
        "id": "sBz6EBVWyh6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_movie, movie_g = prepare_genre(data_movie)\n",
        "data_tv, tv_g = prepare_genre(data_tv)\n",
        "movie_genre = data_movie[movie_g].sum(axis=0)\n",
        "tv_genre = data_tv[tv_g].sum(axis=0)"
      ],
      "metadata": {
        "id": "zgvi9rBxyut2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
        "fig, (ax1, ax2) = plt.subplots(1,2)\n",
        "ax1.pie(movie_genre.values, labels = movie_g)\n",
        "ax2.pie(tv_genre.values, labels = tv_g)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rVw3dmugyw4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
        "plt.pie(movie_genre.values, labels = movie_g)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ikXi86B1yy_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
        "plt.pie(tv_genre.values, labels = tv_g)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gOccDCENy2lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the 10 NaN found in date_added, we use the value one row above to fill it. Because the date_added is sorted in descending order. However, it shows that there are 14 entries which has data_added earlier than release_year, which is impossible. These are invalid inputs, so we decide to delete them.\n",
        "From the user's point of view, the date the movie was added to the library is not important, so we decided not to adopt it."
      ],
      "metadata": {
        "id": "-Y9bRFqBy7Ej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_release_add(dataset):\n",
        "  dataset['date_added'].fillna(method='ffill',inplace=True)\n",
        "  drop_row = dataset[dataset['date_added'].apply(lambda x: x.split(', ')[-1]).astype(int)<dataset['release_year']].index\n",
        "  dataset.drop(drop_row,axis=0,inplace=True)\n",
        "  dataset.drop(columns=['date_added'],axis=1,inplace=True)\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "uEKql6Lly9g-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_movie = prepare_release_add(data_movie)\n",
        "data_tv = prepare_release_add(data_tv)"
      ],
      "metadata": {
        "id": "v2b1Kqlny-J9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}